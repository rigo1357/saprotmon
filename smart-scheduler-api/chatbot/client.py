# smart-scheduler-api/chatbot/client.py

import aiohttp
import asyncio

# Ollama API Configuration
OLLAMA_API_URL = "http://localhost:11434/api/chat"

PROMPT = """
Báº¡n lÃ  **Cá»‘ váº¥n há»c vá»¥ thÃ´ng minh** cá»§a sinh viÃªn TrÆ°á»ng Äáº¡i há»c Thá»§ Dáº§u Má»™t (TDMU).

Nhiá»‡m vá»¥ chÃ­nh cá»§a báº¡n:
- TÆ° váº¥n chá»n mÃ´n há»c theo ngÃ nh, viá»‡n, há»c ká»³.
- Gá»£i Ã½ lá»‹ch há»c tá»‘i Æ°u dá»±a trÃªn thá»i gian ráº£nh, sá»‘ tÃ­n chá»‰, Ä‘á»™ khÃ³ mÃ´n vÃ  cÃ¡c mÃ´n tiÃªn quyáº¿t.
- Giáº£i Ä‘Ã¡p tháº¯c máº¯c vá» há»‡ thá»‘ng Smart Scheduler.
- Há»— trá»£ sinh viÃªn táº¡o thá»i khÃ³a biá»ƒu há»£p lÃ½, trÃ¡nh trÃ¹ng giá», quÃ¡ táº£i hoáº·c vi pháº¡m Ä‘iá»u kiá»‡n tá»‘t nghiá»‡p.

YÃªu cáº§u phong cÃ¡ch:
- Giao tiáº¿p báº±ng **tiáº¿ng Viá»‡t tá»± nhiÃªn**, thÃ¢n thiá»‡n, tinh táº¿.
- Giá»ng vÄƒn giá»‘ng má»™t cá»‘ váº¥n thá»±c sá»±: rÃµ rÃ ng, dá»… hiá»ƒu, chuyÃªn nghiá»‡p nhÆ°ng gáº§n gÅ©i.
- CÃ³ thá»ƒ dÃ¹ng emoji nháº¹ náº¿u phÃ¹ há»£p (ğŸ“š, âœ…, ğŸ’¡).
- KhÃ´ng sá»­ dá»¥ng tiáº¿ng Anh trá»« khi sinh viÃªn yÃªu cáº§u.

Báº¡n pháº£i:
- LuÃ´n Æ°u tiÃªn Ä‘Æ°a ra lá»i khuyÃªn dá»±a trÃªn ngÃ nh â€“ viá»‡n â€“ mÃ´n há»c â€“ lá»‹ch thi â€“ há»c ká»³ â€“ mÃ´n tiÃªn quyáº¿t.
- Khi tÆ° váº¥n, hÃ£y há»i láº¡i thÃ´ng tin cÃ²n thiáº¿u (ngÃ nh, khÃ³a, há»c ká»³, mong muá»‘n cá»§a sinh viÃªn).
- Äá» xuáº¥t lá»‹ch há»c báº±ng cÃ¡ch giáº£i thÃ­ch lÃ½ do há»£p lÃ½.

HÃ£y luÃ´n tráº£ lá»i vá»›i vai trÃ² lÃ  cá»‘ váº¥n há»c vá»¥ há»— trá»£ xáº¿p lá»‹ch thÃ´ng minh.
"""


async def get_bot_response(user_message: str, context_messages: list = None):
    """
    Láº¥y pháº£n há»“i tá»« Ollama (model gemma2:2b).
    
    Args:
        user_message: Tin nháº¯n cá»§a user
        context_messages: Danh sÃ¡ch tin nháº¯n trÆ°á»›c Ä‘Ã³ Ä‘á»ƒ context (optional)
    
    Returns:
        str: Pháº£n há»“i tá»« Ollama AI
    """

    if context_messages is None:
        context_messages = []

    # Táº¡o danh sÃ¡ch messages vá»›i context
    messages = [{"role": "system", "content": PROMPT}]
    messages.extend(context_messages)
    messages.append({"role": "user", "content": user_message})

    # Payload cho Ollama API
    payload = {
        "model": "gemma2:2b",
        "messages": messages,
        "stream": False
    }

    try:
        async with aiohttp.ClientSession() as session:
            try:
                async with session.post(
                    OLLAMA_API_URL, 
                    json=payload, 
                    timeout=aiohttp.ClientTimeout(total=60)
                ) as resp:
                    if resp.status != 200:
                        error_text = await resp.text()
                        return f"Lá»—i server Ollama (status {resp.status}): {error_text[:200]}"

                    data = await resp.json()
                    if "message" in data and "content" in data["message"]:
                        return data["message"]["content"]
                    else:
                        return "KhÃ´ng nháº­n Ä‘Æ°á»£c pháº£n há»“i há»£p lá»‡ tá»« Ollama."
                        
            except asyncio.TimeoutError:
                raise Exception("Timeout: Ollama khÃ´ng pháº£n há»“i trong 60 giÃ¢y. Vui lÃ²ng kiá»ƒm tra Ollama cÃ³ Ä‘ang cháº¡y khÃ´ng.")
            except aiohttp.ClientConnectorError as e:
                raise Exception(f"KhÃ´ng thá»ƒ káº¿t ná»‘i Ä‘áº¿n Ollama táº¡i {OLLAMA_API_URL}. Vui lÃ²ng kiá»ƒm tra Ollama Ä‘Ã£ Ä‘Æ°á»£c khá»Ÿi Ä‘á»™ng chÆ°a (cháº¡y 'ollama serve' trong terminal).")
            except aiohttp.ClientError as e:
                raise Exception(f"Lá»—i káº¿t ná»‘i Ä‘áº¿n Ollama: {str(e)}")

    except Exception as e:
        # Re-raise Ä‘á»ƒ main.py cÃ³ thá»ƒ xá»­ lÃ½
        raise

