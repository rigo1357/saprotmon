# smart-scheduler-api/chatbot/client.py
from openai import OpenAI

# 1. Khá»Ÿi táº¡o Client (sá»­ dá»¥ng 1 láº§n)
client = OpenAI(
    base_url="http://localhost:11434/v1",
    api_key="ollama"  # (Key nÃ y khÃ´ng báº¯t buá»™c vá»›i Ollama)
)

PROMPT = """
Báº¡n lÃ  **Cá»‘ váº¥n há»c vá»¥ thÃ´ng minh** cá»§a sinh viÃªn TrÆ°á»ng Äáº¡i há»c Thá»§ Dáº§u Má»™t (TDMU).

Nhiá»‡m vá»¥ chÃ­nh cá»§a báº¡n:
- TÆ° váº¥n chá»n mÃ´n há»c theo ngÃ nh, viá»‡n, há»c ká»³.
- Gá»£i Ã½ lá»‹ch há»c tá»‘i Æ°u dá»±a trÃªn thá»i gian ráº£nh, sá»‘ tÃ­n chá»‰, Ä‘á»™ khÃ³ mÃ´n vÃ  cÃ¡c mÃ´n tiÃªn quyáº¿t.
- Giáº£i Ä‘Ã¡p tháº¯c máº¯c vá» há»‡ thá»‘ng Smart Scheduler.
- Há»— trá»£ sinh viÃªn táº¡o thá»i khÃ³a biá»ƒu há»£p lÃ½, trÃ¡nh trÃ¹ng giá», quÃ¡ táº£i hoáº·c vi pháº¡m Ä‘iá»u kiá»‡n tá»‘t nghiá»‡p.

YÃªu cáº§u phong cÃ¡ch:
- Giao tiáº¿p báº±ng **tiáº¿ng Viá»‡t tá»± nhiÃªn**, thÃ¢n thiá»‡n, tinh táº¿.
- Giá»ng vÄƒn giá»‘ng má»™t cá»‘ váº¥n thá»±c sá»±: rÃµ rÃ ng, dá»… hiá»ƒu, chuyÃªn nghiá»‡p nhÆ°ng gáº§n gÅ©i.
- CÃ³ thá»ƒ dÃ¹ng emoji nháº¹ náº¿u phÃ¹ há»£p (ğŸ“š, âœ…, ğŸ’¡).
- KhÃ´ng sá»­ dá»¥ng tiáº¿ng Anh trá»« khi sinh viÃªn yÃªu cáº§u.

Báº¡n pháº£i:
- LuÃ´n Æ°u tiÃªn Ä‘Æ°a ra lá»i khuyÃªn dá»±a trÃªn **ngÃ nh â€“ viá»‡n â€“ mÃ´n há»c â€“ lá»‹ch thi â€“ há»c ká»³ â€“ mÃ´n tiÃªn quyáº¿t**.
- Khi tÆ° váº¥n, hÃ£y há»i láº¡i thÃ´ng tin cÃ²n thiáº¿u (ngÃ nh, khÃ³a, há»c ká»³, mong muá»‘n cá»§a sinh viÃªn).
- Äá» xuáº¥t lá»‹ch há»c báº±ng cÃ¡ch giáº£i thÃ­ch lÃ½ do (vÃ­ dá»¥: mÃ´n cÆ¡ sá»Ÿ ngÃ nh nÃªn há»c trÆ°á»›c, trÃ¡nh Ä‘Äƒng kÃ½ nhiá»u mÃ´n náº·ng cÃ¹ng lÃºc).

VÃ­ dá»¥:
Sinh viÃªn: Em muá»‘n xáº¿p lá»‹ch ká»³ 2 ngÃ nh CNTT.
Cá»‘ váº¥n: ÄÆ°á»£c rá»“i, Ä‘á»ƒ mÃ¬nh há»— trá»£ nhÃ©. Ká»³ 2 cá»§a CNTT thÆ°á»ng cÃ³ cÃ¡c mÃ´n cÆ¡ sá»Ÿ nhÆ° Láº­p trÃ¬nh hÆ°á»›ng Ä‘á»‘i tÆ°á»£ng, Kiáº¿n trÃºc mÃ¡y tÃ­nh, ToÃ¡n rá»i ráº¡câ€¦ Báº¡n muá»‘n há»c bao nhiÃªu tÃ­n chá»‰ vÃ  khung giá» nÃ o thÃ¬ thuáº­n tiá»‡n nháº¥t? ğŸ“š

Sinh viÃªn: Em bá»‹ trÃ¹ng giá».
Cá»‘ váº¥n: KhÃ´ng sao, mÃ¬nh sáº½ tÃ¬m nhÃ³m há»c pháº§n khÃ¡c cho báº¡n. HÃ£y gá»­i mÃ¬nh mÃ£ mÃ´n vÃ  nhÃ³m báº¡n Ä‘Ã£ chá»n Ä‘á»ƒ mÃ¬nh kiá»ƒm tra giÃºp nhÃ©. ğŸ’¡

HÃ£y luÃ´n tráº£ lá»i vá»›i vai trÃ² lÃ  cá»‘ váº¥n há»c vá»¥ há»— trá»£ xáº¿p lá»‹ch thÃ´ng minh.
"""


async def get_bot_response(user_message: str, context_messages: list = None):
    """
    Láº¥y pháº£n há»“i tá»« Ollama vá»›i model gemma2:9b (hoáº·c gemma2:2b náº¿u khÃ´ng cÃ³).
    
    Args:
        user_message: Tin nháº¯n cá»§a user
        context_messages: Danh sÃ¡ch tin nháº¯n trÆ°á»›c Ä‘Ã³ Ä‘á»ƒ context (optional)
    
    HÆ¯á»šNG DáºªN Sá»¬ Dá»¤NG:
    1. Äáº£m báº£o Ollama Ä‘Ã£ Ä‘Æ°á»£c cÃ i Ä‘áº·t vÃ  Ä‘ang cháº¡y:
       - Cháº¡y lá»‡nh: ollama serve
    2. Pull model cáº§n thiáº¿t:
       - ollama pull gemma2:9b (khuyáº¿n nghá»‹, cháº¥t lÆ°á»£ng tá»‘t hÆ¡n)
       - hoáº·c: ollama pull gemma2:2b (nháº¹ hÆ¡n, nhanh hÆ¡n)
    3. Náº¿u gáº·p lá»—i connection, kiá»ƒm tra:
       - Ollama Ä‘ang cháº¡y trÃªn port 11434
       - Firewall khÃ´ng cháº·n káº¿t ná»‘i
    """
    if context_messages is None:
        context_messages = []
    
    try:
        # Táº¡o danh sÃ¡ch messages vá»›i context
        messages = [{"role": "system", "content": PROMPT}]
        messages.extend(context_messages)
        messages.append({"role": "user", "content": user_message})
        
        # Thá»­ dÃ¹ng model gemma2:9b trÆ°á»›c (cháº¥t lÆ°á»£ng tá»‘t hÆ¡n)
        # Náº¿u khÃ´ng cÃ³ thÃ¬ sáº½ fallback vá» gemma2:2b
        model_name = "gemma2:9b"
        try:
            response = client.chat.completions.create(
                model=model_name,
                messages=messages,
                temperature=0.8,  # TÄƒng temperature Ä‘á»ƒ pháº£n há»“i tá»± nhiÃªn hÆ¡n
                timeout=60.0  # Timeout 60 giÃ¢y cho model lá»›n hÆ¡n
            )
        except Exception as model_error:
            # Náº¿u model 9b khÃ´ng cÃ³, thá»­ model 2b
            if "model" in str(model_error).lower() or "not found" in str(model_error).lower():
                print(f"Model {model_name} khÃ´ng tÃ¬m tháº¥y, thá»­ gemma2:2b...")
                model_name = "gemma2:2b"
                response = client.chat.completions.create(
                    model=model_name,
                    messages=messages,
                    temperature=0.8,
                    timeout=30.0
                )
            else:
                raise model_error
        
        return response.choices[0].message.content
    except Exception as e:
        print(f"Lá»—i khi gá»i Ollama: {e}")
        error_msg = str(e).lower()
        
        # Kiá»ƒm tra loáº¡i lá»—i vÃ  tráº£ vá» message phÃ¹ há»£p
        if "connection" in error_msg or "refused" in error_msg:
            return "Xin lá»—i, tÃ´i khÃ´ng thá»ƒ káº¿t ná»‘i vá»›i mÃ´ hÃ¬nh AI. Vui lÃ²ng kiá»ƒm tra Ollama Ä‘Ã£ Ä‘Æ°á»£c khá»Ÿi Ä‘á»™ng chÆ°a (cháº¡y 'ollama serve' trong terminal)."
        elif "timeout" in error_msg:
            return "Xin lá»—i, yÃªu cáº§u Ä‘Ã£ háº¿t thá»i gian chá». Vui lÃ²ng thá»­ láº¡i."
        elif "model" in error_msg or "not found" in error_msg:
            return "Xin lá»—i, mÃ´ hÃ¬nh AI chÆ°a Ä‘Æ°á»£c táº£i. Vui lÃ²ng cháº¡y 'ollama pull gemma2:9b' hoáº·c 'ollama pull gemma2:2b' trong terminal."
        else:
            return f"Xin lá»—i, tÃ´i Ä‘ang gáº·p sá»± cá»‘: {str(e)[:100]}. Vui lÃ²ng thá»­ láº¡i sau."