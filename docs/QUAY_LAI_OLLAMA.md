# âœ… ÄÃ£ Quay Láº¡i Sá»­ Dá»¥ng Ollama (Miá»…n PhÃ­)

## ğŸ“‹ TÃ³m Táº¯t

Há»‡ thá»‘ng chatbot Ä‘Ã£ Ä‘Æ°á»£c **restore láº¡i** Ä‘á»ƒ sá»­ dá»¥ng **Ollama** (local AI) thay vÃ¬ DeepSeek API.

## âœ… File ÄÃ£ Thay Äá»•i

### 1. `chatbot/client.py` âœ…
- ÄÃ£ restore láº¡i code Ollama
- Loáº¡i bá» DeepSeek API configuration
- Endpoint: `http://localhost:11434`
- Model: `gemma2:2b`

### 2. `main.py` âœ…
- Cáº­p nháº­t láº¡i error messages Ä‘á»ƒ reference Ollama
- Loáº¡i bá» cÃ¡c error handling cho DeepSeek (401, 429, API key)

### 3. TÃ i Liá»‡u Má»›i ğŸ“š
- `docs/CAI_DAT_OLLAMA.md` - HÆ°á»›ng dáº«n chi tiáº¿t cÃ i Ä‘áº·t Ollama

## ğŸš€ CÃ¡c BÆ°á»›c Tiáº¿p Theo

### BÆ°á»›c 1: CÃ i Äáº·t Ollama

**Windows:**
1. Download táº¡i: https://ollama.com/download
2. Cháº¡y file cÃ i Ä‘áº·t (OllamaSetup.exe)
3. Ollama sáº½ tá»± Ä‘á»™ng cháº¡y

**macOS/Linux:**
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

### BÆ°á»›c 2: Táº£i Model

Má»Ÿ terminal/command prompt vÃ  cháº¡y:
```bash
ollama pull gemma2:2b
```

### BÆ°á»›c 3: Cháº¡y Ollama Server

**Windows:** Ollama tá»± Ä‘á»™ng cháº¡y sau khi cÃ i (check system tray)

**Hoáº·c cháº¡y thá»§ cÃ´ng:**
```bash
ollama serve
```

### BÆ°á»›c 4: Kiá»ƒm Tra

Má»Ÿ browser vÃ  truy cáº­p:
```
http://localhost:11434
```

Báº¡n sáº½ tháº¥y: **"Ollama is running"**

### BÆ°á»›c 5: Test Chatbot

1. Má»Ÿ app Smart Scheduler (http://localhost:3000)
2. VÃ o trang Chatbot
3. Gá»­i tin nháº¯n: "xin chÃ o"
4. Bot sáº½ tráº£ lá»i! ğŸ‰

## ğŸ“Š Æ¯u Äiá»ƒm cá»§a Ollama

âœ… **Miá»…n phÃ­ 100%** - KhÃ´ng tá»‘n tiá»n  
âœ… **Cháº¡y local** - KhÃ´ng cáº§n internet  
âœ… **RiÃªng tÆ°** - Dá»¯ liá»‡u khÃ´ng gá»­i ra ngoÃ i  
âœ… **KhÃ´ng giá»›i háº¡n** - KhÃ´ng cÃ³ rate limit  
âœ… **Offline** - Hoáº¡t Ä‘á»™ng khi máº¥t máº¡ng  

## âš ï¸ YÃªu Cáº§u Há»‡ Thá»‘ng

- **RAM**: Tá»‘i thiá»ƒu 4GB (khuyáº¿n nghá»‹ 8GB)
- **Disk**: ~2GB Ä‘á»ƒ lÆ°u model gemma2:2b
- **CPU**: Báº¥t ká»³ (khÃ´ng cáº§n GPU)

## ğŸ”§ Troubleshooting

### Lá»—i: "KhÃ´ng thá»ƒ káº¿t ná»‘i vá»›i Ollama"

**Kiá»ƒm tra:**
```bash
# Check Ollama cÃ³ cháº¡y khÃ´ng
curl http://localhost:11434

# Náº¿u khÃ´ng cháº¡y, khá»Ÿi Ä‘á»™ng:
ollama serve
```

### Lá»—i: "Model chÆ°a Ä‘Æ°á»£c táº£i"

**Giáº£i phÃ¡p:**
```bash
ollama pull gemma2:2b
ollama list  # Kiá»ƒm tra model Ä‘Ã£ cÃ³
```

## ğŸ“ File Tham Kháº£o

- **HÆ°á»›ng dáº«n chi tiáº¿t**: `docs/CAI_DAT_OLLAMA.md`
- **Code chatbot**: `smart-scheduler-api/chatbot/client.py`
- **Main API**: `smart-scheduler-api/main.py`

## ğŸ”„ So SÃ¡nh: DeepSeek vs Ollama

| Feature | DeepSeek | Ollama |
|---------|----------|--------|
| Chi phÃ­ | CÃ³ phÃ­ ğŸ’° | Miá»…n phÃ­ âœ… |
| Internet | Cáº§n â˜ï¸ | KhÃ´ng cáº§n ğŸ  |
| Tá»‘c Ä‘á»™ | Nhanh | Vá»«a pháº£i |
| Giá»›i háº¡n | Rate limit | KhÃ´ng giá»›i háº¡n |
| Privacy | Cloud | Local âœ… |

## âš¡ Quick Commands

```bash
# CÃ i Ollama (Windows: download tá»« website)
curl -fsSL https://ollama.com/install.sh | sh  # macOS/Linux

# Pull model
ollama pull gemma2:2b

# Cháº¡y server
ollama serve

# Test
ollama run gemma2:2b "xin chÃ o"

# Xem models Ä‘Ã£ cÃ³
ollama list
```

---

**Status:** âœ… Code Ä‘Ã£ Ä‘Æ°á»£c restore vá» Ollama  
**Next Steps:** CÃ i Ä‘áº·t Ollama theo hÆ°á»›ng dáº«n á»Ÿ trÃªn  
**Docs:** Xem `docs/CAI_DAT_OLLAMA.md` Ä‘á»ƒ biáº¿t thÃªm chi tiáº¿t
